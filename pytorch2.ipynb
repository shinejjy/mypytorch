{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115ed877",
   "metadata": {},
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac9134f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import d2lzh_pytorch as d2l\n",
    "import mypytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142e6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    blk = []\n",
    "    for i in range(num_convs):\n",
    "        if i == 0:\n",
    "            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        else:\n",
    "            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        blk.append(nn.ReLU())\n",
    "    blk.append(nn.MaxPool2d(kernel_size=2, stride=2)) # 这里会使宽高减半\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53acf88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = vgg_block(3, 1, 255)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3be428",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch = ((1, 1, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512))\n",
    "# 经过5个vgg_block, 宽高会减半5次, 变成 224/32 = 7\n",
    "fc_features = 512 * 7 * 7 # c * w * h\n",
    "fc_hidden_units = 4096 # 任意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa3506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch, fc_features, fc_hidden_units=4096):\n",
    "    net = nn.Sequential()\n",
    "    # 卷积层部分\n",
    "    for i, (num_convs, in_channels, out_channels) in enumerate(conv_arch):\n",
    "        # 每经过一个vgg_block都会使宽高减半\n",
    "        net.add_module(\"vgg_block_\" + str(i+1), vgg_block(num_convs, in_channels, out_channels))\n",
    "    # 全连接层部分\n",
    "    net.add_module(\"fc\", nn.Sequential(d2l.FlattenLayer(),\n",
    "                                 nn.Linear(fc_features, fc_hidden_units),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.5),\n",
    "                                 nn.Linear(fc_hidden_units, fc_hidden_units),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.5),\n",
    "                                 nn.Linear(fc_hidden_units, 10)\n",
    "                                ))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e8e4fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (vgg_block_1): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_5): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): FlattenLayer()\n",
      "    (1): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = vgg(conv_arch, fc_features, fc_hidden_units)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40d4fa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg_block_1 output shape:  torch.Size([255, 64, 112, 112])\n",
      "vgg_block_2 output shape:  torch.Size([255, 128, 56, 56])\n",
      "vgg_block_3 output shape:  torch.Size([255, 256, 28, 28])\n",
      "vgg_block_4 output shape:  torch.Size([255, 512, 14, 14])\n",
      "vgg_block_5 output shape:  torch.Size([255, 512, 7, 7])\n",
      "fc output shape:  torch.Size([255, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn((255, 1, 224, 224), dtype=torch.float)\n",
    "for name, blk in net.named_children():\n",
    "    X = blk(X)\n",
    "    print(name, 'output shape: ', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b7b1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch = ((1, 1, 2), (1, 2, 4), (2, 4, 8), (2, 8, 16), (2, 16, 32))\n",
    "# 经过5个vgg_block, 宽高会减半5次, 变成 224/32 = 7\n",
    "fc_features = 32 * 7 * 7 # c * w * h\n",
    "fc_hidden_units = 512 # 任意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c7464da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (vgg_block_1): Sequential(\n",
      "    (0): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_2): Sequential(\n",
      "    (0): Conv2d(2, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_3): Sequential(\n",
      "    (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_4): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_5): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): FlattenLayer()\n",
      "    (1): Linear(in_features=1568, out_features=512, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = vgg(conv_arch, fc_features, fc_hidden_units)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dd4b25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练样本集容量: 60000 测试样本集容量: 10000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "# 如出现“out of memory”的报错信息，可减小batch_size或resize\n",
    "train_iter, test_iter = mypytorch.load_data_fashion_mnist(batch_size, resize=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7f9c479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg_block_1 output shape:  torch.Size([255, 2, 112, 112])\n",
      "vgg_block_2 output shape:  torch.Size([255, 4, 56, 56])\n",
      "vgg_block_3 output shape:  torch.Size([255, 8, 28, 28])\n",
      "vgg_block_4 output shape:  torch.Size([255, 16, 14, 14])\n",
      "vgg_block_5 output shape:  torch.Size([255, 32, 7, 7])\n",
      "fc output shape:  torch.Size([255, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn((255, 1, 224, 224), dtype=torch.float)\n",
    "for name, blk in net.named_children():\n",
    "    X = blk(X)\n",
    "    print(name, 'output shape: ', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3727655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m lr, num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.001\u001B[39m, \u001B[38;5;241m5\u001B[39m\n\u001B[0;32m      2\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(net\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr)\n\u001B[1;32m----> 3\u001B[0m l_list \u001B[38;5;241m=\u001B[39m \u001B[43mmypytorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_ch5\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\动手学习深度学习\\mypytorch.py:130\u001B[0m, in \u001B[0;36mtrain_ch5\u001B[1;34m(net, train_iter, test_iter, batch_size, optimizer, device, num_epoch)\u001B[0m\n\u001B[0;32m    128\u001B[0m l \u001B[38;5;241m=\u001B[39m loss(y_hat, y)\n\u001B[0;32m    129\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 130\u001B[0m \u001B[43ml\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    131\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    132\u001B[0m train_l_sum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m l\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "l_list = mypytorch.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39d2d10",
   "metadata": {},
   "source": [
    "-- --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb685c",
   "metadata": {},
   "source": [
    "## NiN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d17599e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import d2lzh_pytorch as d2l\n",
    "import mypytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a9dcb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nin_block(in_channels, out_channels, kernel_size, stride, padding):\n",
    "    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "                        nn.ReLU())\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82b08820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (2): Sequential(\n",
      "    (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Sequential(\n",
      "    (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Dropout(p=0.5, inplace=False)\n",
      "  (7): Sequential(\n",
      "    (0): Conv2d(384, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (8): GlobalAvgPool2d()\n",
      "  (9): FlattenLayer()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nin_block(1, 96, kernel_size=11, stride=4, padding=0),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nin_block(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nin_block(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Dropout(0.5),\n",
    "    # 标签类别数是10\n",
    "    nin_block(384, 10, kernel_size=3, stride=1, padding=1),\n",
    "    mypytorch.GlobalAvgPool2d(),\n",
    "    # 将四维的输出转成二维的输出，其形状为(批量大小, 10)\n",
    "    d2l.FlattenLayer()\n",
    ")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d04bfaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 output shape:  torch.Size([1, 96, 54, 54])\n",
      "1 output shape:  torch.Size([1, 96, 26, 26])\n",
      "2 output shape:  torch.Size([1, 256, 26, 26])\n",
      "3 output shape:  torch.Size([1, 256, 12, 12])\n",
      "4 output shape:  torch.Size([1, 384, 12, 12])\n",
      "5 output shape:  torch.Size([1, 384, 5, 5])\n",
      "6 output shape:  torch.Size([1, 384, 5, 5])\n",
      "7 output shape:  torch.Size([1, 10, 5, 5])\n",
      "8 output shape:  torch.Size([1, 10, 1, 1])\n",
      "9 output shape:  torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 1, 224, 224)\n",
    "for name, blk in net.named_children():\n",
    "    X = blk(X)\n",
    "    print(name, 'output shape: ', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10623d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 2.],\n",
      "          [3., 4.]],\n",
      "\n",
      "         [[2., 3.],\n",
      "          [4., 5.]]],\n",
      "\n",
      "\n",
      "        [[[3., 4.],\n",
      "          [5., 6.]],\n",
      "\n",
      "         [[4., 5.],\n",
      "          [6., 7.]]]])\n",
      "tensor([[[[2., 3.],\n",
      "          [4., 5.]],\n",
      "\n",
      "         [[3., 4.],\n",
      "          [5., 6.]]]])\n",
      "tensor([[[[3., 4.]],\n",
      "\n",
      "         [[4., 5.]]]])\n",
      "tensor([[[[3.5000]],\n",
      "\n",
      "         [[4.5000]]]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[[[1, 2], [3, 4]], [[2, 3], [4, 5]]], [[[3, 4], [5, 6]], [[4, 5], [6, 7]]]], dtype=torch.float)\n",
    "print(X)\n",
    "mean = X.mean(dim=0, keepdim=True)\n",
    "print(mean)\n",
    "mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True)\n",
    "print(mean)\n",
    "mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7bf6e",
   "metadata": {},
   "source": [
    "## BN批量归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1451473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 判断当前模式是训练模式还是预测模式\n",
    "    if not is_training:\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持\n",
    "            # X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "        # 训练模式下用当前的均值和方差做标准化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 拉伸和偏移\n",
    "    return Y, moving_mean, moving_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fba3b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        # 首先要知道是全连接层还是卷积层\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        elif num_dims == 4:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))  # 使用nn.Parameter会自动加入该层的参数中，参与求梯度\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))  # 这里用到的是广播机制\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(self.training,\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95e07d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma torch.Size([1, 100, 1, 1])\n",
      "beta torch.Size([1, 100, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "net = BatchNorm(100, 4)\n",
    "for name, parameter in net.named_parameters():\n",
    "    print(name, parameter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac856a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size\n",
    "            BatchNorm(6, num_dims=4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            BatchNorm(16, num_dims=4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            d2l.FlattenLayer(),\n",
    "            nn.Linear(16*4*4, 120),\n",
    "            BatchNorm(120, num_dims=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            BatchNorm(84, num_dims=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bbdf1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练样本集容量: 60000 测试样本集容量: 10000\n",
      "training on cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [22]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m lr, num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.001\u001B[39m, \u001B[38;5;241m5\u001B[39m\n\u001B[0;32m      5\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(net\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr)\n\u001B[1;32m----> 6\u001B[0m \u001B[43mmypytorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_ch5\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\动手学习深度学习\\mypytorch.py:130\u001B[0m, in \u001B[0;36mtrain_ch5\u001B[1;34m(net, train_iter, test_iter, batch_size, optimizer, device, num_epoch)\u001B[0m\n\u001B[0;32m    128\u001B[0m l \u001B[38;5;241m=\u001B[39m loss(y_hat, y)\n\u001B[0;32m    129\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 130\u001B[0m \u001B[43ml\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    131\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    132\u001B[0m train_l_sum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m l\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = mypytorch.load_data_fashion_mnist(batch_size=batch_size)\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "mypytorch.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d41bde94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([1.0481, 1.0286, 0.9637, 0.9313, 0.9937, 1.0273],\n        grad_fn=<ViewBackward0>),\n tensor([ 0.1078, -0.1014, -0.0463, -0.0195,  0.0102,  0.0519],\n        grad_fn=<ViewBackward0>))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].gamma.view((-1,)), net[1].beta.view((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4626fd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): Sigmoid()\n",
      "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): Sigmoid()\n",
      "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (8): FlattenLayer()\n",
      "  (9): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (10): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (11): Sigmoid()\n",
      "  (12): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (13): BatchNorm1d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (14): Sigmoid()\n",
      "  (15): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            d2l.FlattenLayer(),\n",
    "            nn.Linear(16*4*4, 120),\n",
    "            nn.BatchNorm1d(120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.BatchNorm1d(84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "222505ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练样本集容量: 60000 测试样本集容量: 10000\n",
      "training on cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [25]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m lr, num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.001\u001B[39m, \u001B[38;5;241m5\u001B[39m\n\u001B[0;32m      5\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(net\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr)\n\u001B[1;32m----> 6\u001B[0m \u001B[43mmypytorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_ch5\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\动手学习深度学习\\mypytorch.py:130\u001B[0m, in \u001B[0;36mtrain_ch5\u001B[1;34m(net, train_iter, test_iter, batch_size, optimizer, device, num_epoch)\u001B[0m\n\u001B[0;32m    128\u001B[0m l \u001B[38;5;241m=\u001B[39m loss(y_hat, y)\n\u001B[0;32m    129\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 130\u001B[0m \u001B[43ml\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    131\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    132\u001B[0m train_l_sum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m l\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = mypytorch.load_data_fashion_mnist(batch_size=batch_size)\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "mypytorch.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "-- --"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 残差网络 ResNet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import d2lzh_pytorch as d2l\n",
    "import mypytorch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 3, 6, 6])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn((4, 3, 6, 6), dtype=torch.float)\n",
    "blk = mypytorch.Residual(3, 3)\n",
    "blk(X).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 6, 2, 2])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = mypytorch.Residual(3, 6, use_1x1conv=True, stride=3)\n",
    "blk(X).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "        nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def renet_block(in_channels, out_channels, num_residuals, first_block=False):\n",
    "    if first_block:\n",
    "        assert in_channels == out_channels\n",
    "    blks = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blks.append(mypytorch.Residual(in_channels, out_channels, use_1x1conv=True, stride=2))\n",
    "        else:\n",
    "            blks.append(mypytorch.Residual(out_channels, out_channels))\n",
    "    return nn.Sequential(*blks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (resnet_block1): Sequential(\n",
      "    (0): Residual(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Residual(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (resnet_block2): Sequential(\n",
      "    (0): Residual(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Residual(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (resnet_block3): Sequential(\n",
      "    (0): Residual(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Residual(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (resnet_block4): Sequential(\n",
      "    (0): Residual(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Residual(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (global_avg_pool): GlobalAvgPool2d()\n",
      "  (fc): Sequential(\n",
      "    (0): FlattenLayer()\n",
      "    (1): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net.add_module('resnet_block1', renet_block(64, 64, 2, first_block=True))\n",
    "net.add_module('resnet_block2', renet_block(64, 128, 2))\n",
    "net.add_module('resnet_block3', renet_block(128, 256, 2))\n",
    "net.add_module('resnet_block4', renet_block(256, 512, 2))\n",
    "net.add_module('global_avg_pool', mypytorch.GlobalAvgPool2d())\n",
    "net.add_module('fc', nn.Sequential(mypytorch.FlattenLayer(), nn.Linear(512, 10)))\n",
    "print(net)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 64, 112, 112])\n",
      "1 torch.Size([1, 64, 112, 112])\n",
      "2 torch.Size([1, 64, 112, 112])\n",
      "3 torch.Size([1, 64, 56, 56])\n",
      "resnet_block1 torch.Size([1, 64, 56, 56])\n",
      "resnet_block2 torch.Size([1, 128, 28, 28])\n",
      "resnet_block3 torch.Size([1, 256, 14, 14])\n",
      "resnet_block4 torch.Size([1, 512, 7, 7])\n",
      "global_avg_pool torch.Size([1, 512, 1, 1])\n",
      "fc torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn((1, 1, 224, 224))\n",
    "for name, blk in net.named_children():\n",
    "    X = blk(X)\n",
    "    print(name, X.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练样本集容量: 60000 测试样本集容量: 10000\n",
      "training on cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [33]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m lr, num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.001\u001B[39m, \u001B[38;5;241m5\u001B[39m\n\u001B[0;32m      7\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(net\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr)\n\u001B[1;32m----> 8\u001B[0m \u001B[43mmypytorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_ch5\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\动手学习深度学习\\mypytorch.py:127\u001B[0m, in \u001B[0;36mtrain_ch5\u001B[1;34m(net, train_iter, test_iter, batch_size, optimizer, device, num_epoch)\u001B[0m\n\u001B[0;32m    125\u001B[0m X \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    126\u001B[0m y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m--> 127\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    128\u001B[0m l \u001B[38;5;241m=\u001B[39m loss(y_hat, y)\n\u001B[0;32m    129\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\动手学习深度学习\\mypytorch.py:184\u001B[0m, in \u001B[0;36mResidual.forward\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m    183\u001B[0m     Y \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn1(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv1(X)))\n\u001B[1;32m--> 184\u001B[0m     Y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn2(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mY\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    185\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv3:\n\u001B[0;32m    186\u001B[0m         X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv3(X)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    456\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 457\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    449\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    450\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    451\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    452\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 453\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    454\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 256\n",
    "# 如出现“out of memory”的报错信息，可减小batch_size或resize\n",
    "train_iter, test_iter = mypytorch.load_data_fashion_mnist(batch_size, resize=96)\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "mypytorch.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-- --\n",
    "# 循环神经网络"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.7725,  3.0355, -1.4164,  0.4598],\n        [ 0.2035,  0.0279,  4.1969,  0.3503],\n        [ 2.0789, -1.9873, -1.7043, -0.5002]])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X, W_xh = torch.randn(3, 1), torch.randn(1, 4)\n",
    "H, W_hh = torch.randn(3, 4), torch.randn(4, 4)\n",
    "torch.matmul(X, W_xh) + torch.matmul(H, W_hh)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.7725,  3.0355, -1.4164,  0.4598],\n        [ 0.2035,  0.0279,  4.1969,  0.3503],\n        [ 2.0789, -1.9873, -1.7043, -0.5002]])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.cat((X, H), dim=1), torch.cat((W_xh, W_hh), dim=0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 600x300 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mypytorch\n",
    "T = 1000  # 总共产生1000个点\n",
    "time = torch.arange(1, T + 1, dtype=torch.float32)\n",
    "x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,))\n",
    "mypytorch.plot(time, x, 'time', 'x', xlim=[1, 1000], figsize=(6, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "'想要有直升机\\n想要和你飞到宇宙去\\n想要和你融化在一起\\n融化在宇宙里\\n我每天每天每'"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('data/jaychou_lyrics.txt.zip') as zin:\n",
    "    with zin.open('jaychou_lyrics.txt') as f:\n",
    "        corpus_chars = f.read().decode('utf-8')\n",
    "corpus_chars[:40]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[0:10000]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "1027"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "vocab_size # 1027"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: 想要有直升机 想要和你飞到宇宙去 想要和\n",
      "indices: [809, 6, 323, 661, 142, 739, 720, 809, 6, 527, 464, 56, 257, 904, 870, 486, 720, 809, 6, 527]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "sample = corpus_indices[:20]\n",
    "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('indices:', sample)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import mypytorch as mp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 21, 1, 15, 28, 12, 26, 19, 2, 6, 17, 23, 16, 29, 4, 10, 11, 20, 22, 27, 9, 18, 24, 7, 13, 25, 8, 5, 3, 0]\n",
      "\n",
      "X: tensor([[16., 29.,  4., 10., 11., 20.],\n",
      "        [22., 27.,  9., 18., 24.,  7.]]) \n",
      "Y: tensor([[29.,  4., 10., 11., 20., 22.],\n",
      "        [27.,  9., 18., 24.,  7., 13.]])\n",
      "\n",
      "X: tensor([[14., 21.,  1., 15., 28., 12.],\n",
      "        [26., 19.,  2.,  6., 17., 23.]]) \n",
      "Y: tensor([[21.,  1., 15., 28., 12., 26.],\n",
      "        [19.,  2.,  6., 17., 23., 16.]])\n"
     ]
    }
   ],
   "source": [
    "corpus_indices_test = list(range(30))\n",
    "random.shuffle(corpus_indices_test)\n",
    "print(corpus_indices_test)\n",
    "for X,Y in mp.data_iter_random(corpus_indices_test, 2, 6):\n",
    "    print('\\nX:', X, '\\nY:', Y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 2, 19, 26, 16, 28, 18, 29, 8, 23, 21, 7, 4, 5, 14, 1, 10, 6, 0, 25, 22, 24, 11, 27, 20, 12, 15, 3, 9, 13]\n",
      "\n",
      "X: tensor([[17.,  2., 19., 26., 16., 28.],\n",
      "        [ 1., 10.,  6.,  0., 25., 22.]]) \n",
      "Y: tensor([[ 2., 19., 26., 16., 28., 18.],\n",
      "        [10.,  6.,  0., 25., 22., 24.]])\n",
      "\n",
      "X: tensor([[18., 29.,  8., 23., 21.,  7.],\n",
      "        [24., 11., 27., 20., 12., 15.]]) \n",
      "Y: tensor([[29.,  8., 23., 21.,  7.,  4.],\n",
      "        [11., 27., 20., 12., 15.,  3.]])\n"
     ]
    }
   ],
   "source": [
    "corpus_indices_test = list(range(30))\n",
    "random.shuffle(corpus_indices_test)\n",
    "print(corpus_indices_test)\n",
    "for X,Y in mp.data_iter_consecutive(corpus_indices_test, 2, 6):\n",
    "    print('\\nX:', X, '\\nY:', Y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "####  代码实现"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import mypytorch as mp\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "corpus_indices, char_to_idx, idx_to_char, vocab_size = mp.load_data_jay_lyrics()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 torch.Size([2, 1027])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(10).view(2, 5)\n",
    "inputs = mp.to_onehot(X, vocab_size)\n",
    "print(len(inputs), inputs[0].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def get_params(num_inputs, num_hiddens, num_outputs, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # 隐藏层参数\n",
    "    W_xh = mp._one((num_inputs, num_hiddens), device)\n",
    "    W_hh = mp._one((num_hiddens, num_hiddens), device)\n",
    "    b_h = nn.Parameter(torch.zeros(num_hiddens, device=device, requires_grad=True))\n",
    "    # 输出层参数\n",
    "    W_hq = mp._one((num_hiddens, num_outputs), device)\n",
    "    b_q = nn.Parameter(torch.zeros(num_outputs, device=device, requires_grad=True))\n",
    "    return nn.ParameterList([W_xh, W_hh, b_h, W_hq, b_q])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)\n",
    "        Y = torch.matmul(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, (H,)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 torch.Size([2, 1027]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "\n",
    "state = init_rnn_state(X.shape[0], num_hiddens, device)\n",
    "inputs = mp.to_onehot(X.to(device), vocab_size)\n",
    "params = get_params(num_inputs, num_hiddens, num_outputs, device)\n",
    "outputs, state_new = rnn(inputs, state, params)\n",
    "print(len(outputs), outputs[0].shape, state_new[0].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "'分开球温秃亮诉威狼溪伤临'"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.predict_rnn('分开', 10, mp.rnn, params,mp.init_rnn_state, num_hiddens, vocab_size, device, idx_to_char, char_to_idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11],\n         [12, 13, 14, 15]]),\n tensor([[ 1,  2,  3,  4],\n         [ 5,  6,  7,  8],\n         [ 9, 10, 11, 12],\n         [13, 14, 15, 16]])]"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16).view(4, 4)\n",
    "X = [X, X + 1]\n",
    "X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15],\n        [ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12],\n        [13, 14, 15, 16]])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(X, dim=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 随机采样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 69.833313, time 0.69 sec\n",
      " - 分开 我不要再想 我不儿 想 我不 我想 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我\n",
      " - 不分开 我想要 说 我不儿 想 我不 我想 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我\n",
      "epoch 100, perplexity 10.198107, time 0.68 sec\n",
      " - 分开 一只了不 我有没有你 我不能开  我爱上你想你 不要我 你知是 我不红 一小四三 一壶忆  在我抬\n",
      " - 不分开 一颗两步 在我有脚的溪边 默默等待 一颗村 在片上 一壶四人 一漠忆 在片我 娘子 有我 回子 有\n",
      "epoch 150, perplexity 2.999162, time 1.28 sec\n",
      " - 分开 一颗心不截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 如果我有轻功 飞檐走壁 为人用直不棍 一身正气 快\n",
      " - 不分开扫 我叫你爸 你打我妈 这样出吗的我 相思寄红豆 相思寄红豆 有分不要走 有话人开走 有话在不落 有\n",
      "epoch 200, perplexity 1.618452, time 0.70 sec\n",
      " - 分开 你想我 你怎么 看不好人已江不是一只到演 在小村外的溪边河口 默默中向开始移动 回说当初爱你的时空\n",
      " - 不分开简简单单没有伤害 你 靠着我的肩膀 你 在我胸口睡著 像这样的生活 我爱你 你爱我 开不了口 周杰伦\n",
      "epoch 250, perplexity 1.319999, time 1.22 sec\n",
      " - 分开 你想心空哭 我后懂里 在家怕空出 白色蜡烛 温暖了空屋 白色蜡烛 温暖了空屋 白色蜡烛 温暖了空屋\n",
      " - 不分开扫 然后将过去 慢慢温习 让我爱上你 那场悲剧 是你完美演出的一场戏 宁愿心碎哭泣 再狠狠忘记 你爱\n"
     ]
    }
   ],
   "source": [
    "mp.train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      vocab_size, device, corpus_indices, idx_to_char,\n",
    "                      char_to_idx, True, num_epochs, num_steps, lr,\n",
    "                      clipping_theta, batch_size, pred_period, pred_len,\n",
    "                      prefixes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 相邻采样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 57.651260, time 0.71 sec\n",
      " - 分开 我不要再 一使了双 在谁的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 \n",
      " - 不分开 我有就的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏\n",
      "epoch 100, perplexity 6.743055, time 0.72 sec\n",
      " - 分开 我想要这样想 你知不了我 我有要有想 我不 我不 我不要再想你 不情再的太快就像龙卷风 离不开 爱\n",
      " - 不分开柳 你已经黑 我想要难我 相没的有 全小晶空切 白色蜡烛 温暖了空屋 白色蜡烛 温暖了空屋 白色蜡烛\n",
      "epoch 150, perplexity 2.041161, time 0.66 sec\n",
      " - 分开 我不要这样活 唱着我遇见你是一场悲剧 我想我这辈子注定一个人演戏 最后再一个人慢慢的回忆 我有你这\n",
      " - 不分开觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生\n",
      "epoch 200, perplexity 1.278854, time 1.22 sec\n",
      " - 分开 我都多 如果我的三有一亏 隔果底国 我想一难熬  没有你在我有多难熬多烦恼  没有你烦 我有多烦恼\n",
      " - 不分开觉 你已经直了很 爸生后起 后知风一个秋  我的世界已狂风暴雨 Wu  爱情来的太快就像龙卷风 离不\n",
      "epoch 250, perplexity 1.159463, time 0.80 sec\n",
      " - 分开 我也多 如果我 印地安的 说 我有想回不听 连隔壁邻居都猜到我现在的感受 河边的风 在吹着头发飘动\n",
      " - 不分开觉 你已经离了我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生\n"
     ]
    }
   ],
   "source": [
    "mp.train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      vocab_size, device, corpus_indices, idx_to_char,\n",
    "                      char_to_idx, False, num_epochs, num_steps, lr,\n",
    "                      clipping_theta, batch_size, pred_period, pred_len,\n",
    "                      prefixes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 简洁实现"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import mypytorch as mp\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = mp.load_data_jay_lyrics(20000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "num_steps = 35\n",
    "batch_size = 2\n",
    "state = None\n",
    "X = torch.rand(num_steps, batch_size, vocab_size)\n",
    "Y, state_new = rnn_layer(X, state)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'分开脆居居居居居居居居居'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mp.RNNModel(rnn_layer, vocab_size).to(device)\n",
    "mp.predict_rnn_pytorch('分开', 10, model, vocab_size, device, idx_to_char, char_to_idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 1.049160, time 1.02 sec\n",
      " - 分开始使用水在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你\n",
      " - 不分开始使用水在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你在你\n",
      "epoch 100, perplexity 1.043067, time 1.06 sec\n",
      " - 分开始使用水在盒的勇气 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 \n",
      " - 不分开始使用水在盒的勇气 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 \n",
      "epoch 150, perplexity 1.041902, time 1.07 sec\n",
      " - 分开始使用水在盒的勇气 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 \n",
      " - 不分开始使用水在盒的勇气 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 \n",
      "epoch 200, perplexity 1.041342, time 1.07 sec\n",
      " - 分开始使用水在盒的时候你在盒的时候你在盒的时候你在盒的时候你在盒的时候你在盒的时候你在盒的时候你在盒的时\n",
      " - 不分开始使用水在盒的时候你在盒的时候你在盒的时候你在盒的时候你在盒的时候你在盒的时候你在盒的时候你在盒的时\n",
      "epoch 250, perplexity 1.041001, time 1.05 sec\n",
      " - 分开始使用水在笑 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回\n",
      " - 不分开始使用水在笑 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回 我想回\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 500x250 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"342.70625pt\" height=\"183.35625pt\" viewBox=\"0 0 342.70625 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-01-13T17:47:39.387125</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 342.70625 183.35625 \nL 342.70625 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 56.50625 145.8 \nL 335.50625 145.8 \nL 335.50625 7.2 \nL 56.50625 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m88418bc7b4\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m88418bc7b4\" x=\"69.188068\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 50 -->\n      <g transform=\"translate(62.825568 160.398438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m88418bc7b4\" x=\"100.892614\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 75 -->\n      <g transform=\"translate(94.530114 160.398438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m88418bc7b4\" x=\"132.597159\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <g transform=\"translate(123.053409 160.398438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m88418bc7b4\" x=\"164.301705\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 125 -->\n      <g transform=\"translate(154.757955 160.398438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m88418bc7b4\" x=\"196.00625\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 150 -->\n      <g transform=\"translate(186.4625 160.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m88418bc7b4\" x=\"227.710795\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 175 -->\n      <g transform=\"translate(218.167045 160.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m88418bc7b4\" x=\"259.415341\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 200 -->\n      <g transform=\"translate(249.871591 160.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m88418bc7b4\" x=\"291.119886\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 225 -->\n      <g transform=\"translate(281.576136 160.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m88418bc7b4\" x=\"322.824432\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 250 -->\n      <g transform=\"translate(313.280682 160.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- epoch -->\n     <g transform=\"translate(180.778125 174.076563)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path id=\"m8b90c521e5\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m8b90c521e5\" x=\"56.50625\" y=\"124.078442\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.042 -->\n      <g transform=\"translate(20.878125 127.877661)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"222.65625\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m8b90c521e5\" x=\"56.50625\" y=\"93.189027\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.044 -->\n      <g transform=\"translate(20.878125 96.988246)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"222.65625\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m8b90c521e5\" x=\"56.50625\" y=\"62.299612\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.046 -->\n      <g transform=\"translate(20.878125 66.098831)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"222.65625\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#m8b90c521e5\" x=\"56.50625\" y=\"31.410197\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.048 -->\n      <g transform=\"translate(20.878125 35.209415)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"222.65625\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- perplexity -->\n     <g transform=\"translate(14.798438 101.626563)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"63.476562\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"166.113281\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"229.589844\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"257.373047\"/>\n      <use xlink:href=\"#DejaVuSans-78\" x=\"317.146484\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"376.326172\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"404.109375\"/>\n      <use xlink:href=\"#DejaVuSans-79\" x=\"443.318359\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\"/>\n   <g id=\"line2d_15\"/>\n   <g id=\"line2d_16\"/>\n   <g id=\"line2d_17\"/>\n   <g id=\"line2d_18\"/>\n   <g id=\"line2d_19\"/>\n   <g id=\"line2d_20\"/>\n   <g id=\"line2d_21\"/>\n   <g id=\"line2d_22\"/>\n   <g id=\"line2d_23\"/>\n   <g id=\"line2d_24\"/>\n   <g id=\"line2d_25\"/>\n   <g id=\"line2d_26\"/>\n   <g id=\"line2d_27\"/>\n   <g id=\"line2d_28\"/>\n   <g id=\"line2d_29\"/>\n   <g id=\"line2d_30\"/>\n   <g id=\"line2d_31\"/>\n   <g id=\"line2d_32\"/>\n   <g id=\"line2d_33\"/>\n   <g id=\"line2d_34\"/>\n   <g id=\"line2d_35\"/>\n   <g id=\"line2d_36\"/>\n   <g id=\"line2d_37\"/>\n   <g id=\"line2d_38\"/>\n   <g id=\"line2d_39\"/>\n   <g id=\"line2d_40\"/>\n   <g id=\"line2d_41\"/>\n   <g id=\"line2d_42\"/>\n   <g id=\"line2d_43\"/>\n   <g id=\"line2d_44\"/>\n   <g id=\"line2d_45\"/>\n   <g id=\"line2d_46\"/>\n   <g id=\"line2d_47\"/>\n   <g id=\"line2d_48\"/>\n   <g id=\"line2d_49\"/>\n   <g id=\"line2d_50\"/>\n   <g id=\"line2d_51\"/>\n   <g id=\"line2d_52\"/>\n   <g id=\"line2d_53\"/>\n   <g id=\"line2d_54\"/>\n   <g id=\"line2d_55\"/>\n   <g id=\"line2d_56\"/>\n   <g id=\"line2d_57\"/>\n   <g id=\"line2d_58\"/>\n   <g id=\"line2d_59\"/>\n   <g id=\"line2d_60\"/>\n   <g id=\"line2d_61\"/>\n   <g id=\"line2d_62\"/>\n   <g id=\"line2d_63\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_64\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_65\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_66\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_67\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_68\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_69\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_70\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_71\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_72\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_73\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_74\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_75\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_76\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_77\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_78\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_79\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_80\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_81\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_82\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_83\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_84\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_85\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_86\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_87\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_88\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_89\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_90\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_91\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_92\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_93\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_94\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_95\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_96\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_97\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_98\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_99\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_100\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_101\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_102\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_103\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_104\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_105\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_106\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_107\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_108\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_109\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_110\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_111\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_112\">\n    <path d=\"M 69.188068 13.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_113\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_114\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_115\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_116\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_117\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_118\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_119\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_120\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_121\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_122\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_123\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_124\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_125\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_126\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_127\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_128\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_129\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_130\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_131\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_132\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_133\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_134\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_135\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_136\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_137\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_138\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_139\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_140\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_141\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_142\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_143\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_144\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_145\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_146\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_147\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_148\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_149\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_150\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_151\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_152\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_153\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_154\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_155\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_156\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_157\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_158\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_159\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_160\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_161\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_162\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_163\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_164\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_165\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_166\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_167\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_168\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_169\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_170\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_171\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_172\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_173\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_174\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_175\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_176\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_177\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_178\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_179\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_180\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_181\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_182\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_183\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_184\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_185\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_186\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_187\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_188\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_189\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_190\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_191\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_192\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_193\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_194\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_195\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_196\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_197\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_198\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_199\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_200\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_201\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_202\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_203\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_204\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_205\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_206\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_207\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_208\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_209\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_210\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_211\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_212\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_213\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_214\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_215\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_216\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_217\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_218\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_219\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_220\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_221\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_222\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_223\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_224\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_225\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_226\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_227\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_228\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_229\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_230\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_231\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_232\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_233\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_234\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_235\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_236\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_237\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_238\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_239\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_240\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_241\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_242\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_243\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_244\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_245\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_246\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_247\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_248\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_249\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_250\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_251\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_252\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_253\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_254\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_255\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_256\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_257\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_258\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_259\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_260\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_261\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_262\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #bcbd22; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_263\">\n    <path d=\"M 69.188068 13.5 \nL 132.597159 107.602602 \nL 196.00625 125.586274 \nL 259.415341 134.241115 \nL 322.824432 139.5 \n\" clip-path=\"url(#pe4bbb671e2)\" style=\"fill: none; stroke: #17becf; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 56.50625 145.8 \nL 56.50625 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 335.50625 145.8 \nL 335.50625 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 56.50625 145.8 \nL 335.50625 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 56.50625 7.2 \nL 335.50625 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe4bbb671e2\">\n   <rect x=\"56.50625\" y=\"7.2\" width=\"279\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps, num_hiddens, num_epochs, batch_size, lr, clipping_theta  = 35, 256, 250, 32, 1e-3, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']\n",
    "mp.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device, corpus_indices, idx_to_char, char_to_idx, num_epochs,\n",
    "                                 num_steps, lr, clipping_theta, batch_size, pred_period, pred_len, prefixes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
